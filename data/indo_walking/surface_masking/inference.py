## inference.py 

import torch
import torch.nn.functional as F
import numpy as np
import torch.nn as nn
from PIL import Image
import os
import cv2 # í…ìŠ¤íŠ¸ ì¶”ê°€ë¥¼ ìœ„í•´ OpenCV ì„í¬íŠ¸
from transformers import (
    SegformerImageProcessor,
    SegformerForSemanticSegmentation,
    TrainingArguments,
    Trainer
)

import json
import cv2

from torch.utils.data import Dataset, DataLoader

# from surface_dataset import SurfaceSegmentationDataset

# surface_dataset.py - ìˆ˜ì •ëœ ë²„ì „

import torch
from torch.utils.data import Dataset
from torchvision import transforms
import cv2
import numpy as np
from typing import List, Dict, Tuple

class SurfaceSegmentationDataset(Dataset):
    """ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€/ë§ˆìŠ¤í¬ ê²½ë¡œë¥¼ ë°›ì•„ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, data_list: List[Dict], target_size: Tuple[int, int], is_train: bool):
        self.data_list = data_list
        self.target_size = target_size
        self.is_train = is_train

        # ì´ë¯¸ì§€ ë³€í™˜ - ì •ê·œí™” ì œê±° (DirectSegFormerì—ì„œ ì²˜ë¦¬)
        if self.is_train:
            self.image_transform = transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize(self.target_size),
                # ì„¸ê·¸ë©˜í…Œì´ì…˜ì—ì„œëŠ” ìƒ‰ìƒ ë³€í™”ë¥¼ ìµœì†Œí™”
                transforms.ColorJitter(brightness=0.1, contrast=0.1),  # ìƒ‰ìƒ ë³€í™” ìµœì†Œí™”
                # transforms.RandomHorizontalFlip(p=0.3),  # í™•ë¥  ê°ì†Œ
                transforms.ToTensor(),  # 0-1 ë²”ìœ„ë¡œë§Œ ë³€í™˜
                # ì •ê·œí™” ì œê±°! DirectSegFormerì—ì„œ ì²˜ë¦¬
            ])
        else:
            self.image_transform = transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize(self.target_size),
                transforms.ToTensor(),  # 0-1 ë²”ìœ„ë¡œë§Œ ë³€í™˜
                # ì •ê·œí™” ì œê±°!
            ])

        # ë§ˆìŠ¤í¬ ë³€í™˜ - ìˆ˜í‰ ë’¤ì§‘ê¸° ì¶”ê°€ (ì´ë¯¸ì§€ì™€ ë™ì¼í•˜ê²Œ)
        if self.is_train:
            self.mask_transform = transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize(self.target_size, interpolation=transforms.InterpolationMode.NEAREST),
                # transforms.RandomHorizontalFlip(p=0.3),  # ì´ë¯¸ì§€ì™€ ë™ì¼í•œ í™•ë¥ 
                transforms.ToTensor()
            ])
        else:
            self.mask_transform = transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize(self.target_size, interpolation=transforms.InterpolationMode.NEAREST),
                transforms.ToTensor()
            ])

    def __len__(self) -> int:
        return len(self.data_list)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        # metadata.jsonì— ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ì½ê¸°
        item = self.data_list[idx]
        image_path = item['image_path']
        mask_path = item['mask_path']

        # ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ ë¡œë“œ
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        # ë™ì¼í•œ ì‹œë“œë¡œ ëœë¤ ë³€í™˜ ì ìš© (ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ê°€ ê°™ì€ ë³€í™˜ì„ ë°›ë„ë¡)
        if self.is_train:
            seed = torch.randint(0, 2**32, (1,)).item()
            torch.manual_seed(seed)
            image = self.image_transform(image)
            torch.manual_seed(seed)
            mask = self.mask_transform(mask)
        else:
            image = self.image_transform(image)
            mask = self.mask_transform(mask)

        # ë§ˆìŠ¤í¬ í…ì„œ í˜•íƒœ ë³€ê²½: (1, H, W) -> (H, W), Long íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        mask = mask.squeeze(0).long()

        return {'pixel_values': image, 'labels': mask}

# --- ì„¤ì • ---
# ì „ì²˜ë¦¬ëœ ë°ì´í„° ì •ë³´ê°€ ë‹´ê¸´ metadata.json íŒŒì¼ ê²½ë¡œ
metadata_path = "./processed_dataset/metadata.json"

# í•™ìŠµ íŒŒë¼ë¯¸í„°
batch_size = 32
target_size = (512, 512)
num_workers = 16  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì ˆ

"""
metadata.jsonì„ ì½ì–´ í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¡œë”ë¥¼ ìƒì„±
"""
with open(metadata_path, 'r', encoding='utf-8') as f:
    metadata = json.load(f)

# ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
valid_dataset = SurfaceSegmentationDataset(metadata['valid_data'], target_size, is_train=False)

valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)


# --- ì‚¬ìš©ì ì„¤ì • ---
# ë¹„êµí•  ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬)
MODEL_PATHS = [
    "ckpts/seg_model_epoch_1.pth",   # ì˜ˆ: ì´ˆê¸° epoch
    "ckpts/seg_model_epoch_5.pth",   # ì˜ˆ: ì¤‘ê°„ epoch
    "ckpts/seg_model_epoch_30.pth"   # ì˜ˆ: ìµœì¢… epoch
]
# ê° ëª¨ë¸ì— í•´ë‹¹í•˜ëŠ” ë¼ë²¨
EPOCH_LABELS = ["Epoch 1", "Epoch 5", "Epoch 30"]

# ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬
OUTPUT_DIR = "inference_progress_results"
# ì €ì¥í•  ì´ë¯¸ì§€ ê°œìˆ˜
NUM_IMAGES_TO_SAVE = 10
# GPU ì„¤ì •
DEVICE = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")

# --- ì´ì „ ì½”ë“œì˜ í´ë˜ìŠ¤ ë° ë³€ìˆ˜ ì •ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤ ---
# DirectSegFormer ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
class DirectSegFormer(nn.Module):
    def __init__(self, pretrained_model_name="nvidia/mit-b0", num_classes=7):
        super().__init__()
        self.original_model = SegformerForSemanticSegmentation.from_pretrained(
            pretrained_model_name,
            num_labels=num_classes,
            ignore_mismatched_sizes=True
        )
        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))
    def forward(self, x):
        x = (x - self.mean) / self.std
        outputs = self.original_model(pixel_values=x)
        return outputs.logits

# --- ì¶”ë¡  ë° ì‹œê°í™” ì½”ë“œ ---

# 1. ì„¤ì • ê²€ì¦ ë° ë””ë ‰í„°ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"ê²°ê³¼ ì´ë¯¸ì§€ëŠ” '{OUTPUT_DIR}' ë””ë ‰í„°ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.")

for path in MODEL_PATHS:
    if not os.path.exists(path):
        print(f"ğŸš¨ ì—ëŸ¬: ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ -> {path}")
        exit()

# 2. ì‹œê°í™”ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
palette = {0: (0, 0, 0), 1: (255, 255, 0), 2: (0, 255, 0), 3: (100, 100, 100), 4: (255, 0, 0), 5: (0, 0, 255), 6: (255, 0, 255)}

def mask_to_rgb(mask_np, color_palette):
    rgb_mask = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)
    for class_idx, color in color_palette.items():
        rgb_mask[mask_np == class_idx] = color
    return rgb_mask

def add_label_to_image(image, label):
    """OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ ë¼ë²¨ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜"""
    # PIL ì´ë¯¸ì§€ë¥¼ OpenCV í¬ë§·(BGR)ìœ¼ë¡œ ë³€í™˜
    img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    # í…ìŠ¤íŠ¸ ì„¤ì •
    font = cv2.FONT_HERSHEY_SIMPLEX
    position = (10, 30)  # ì´ë¯¸ì§€ ì¢Œì¸¡ ìƒë‹¨ ì¢Œí‘œ
    font_scale = 1
    font_color = (255, 255, 255)  # í°ìƒ‰
    thickness = 2
    # ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ ì¶”ê°€
    cv2.putText(img_cv, label, position, font, font_scale, font_color, thickness, cv2.LINE_AA)
    # ë‹¤ì‹œ PIL í¬ë§·(RGB)ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    return Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))

# 3. ëª¨ë¸ êµ¬ì¡° ë¡œë“œ (ê°€ì¤‘ì¹˜ëŠ” ë£¨í”„ ì•ˆì—ì„œ êµì²´)
model = DirectSegFormer(num_classes=7)
model.to(DEVICE)
model.eval()

# 4. ì¶”ë¡  ë° ì´ë¯¸ì§€ ìƒì„± ë£¨í”„
saved_count = 0
with torch.no_grad():
    for batch_data in valid_loader:
        if saved_count >= NUM_IMAGES_TO_SAVE:
            break

        # ë°°ì¹˜ì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©
        image_tensor = batch_data['pixel_values'][0].unsqueeze(0).to(DEVICE)
        gt_mask_tensor = batch_data['labels'][0]

        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        prediction_images = []

        # ì§€ì •ëœ ê° ëª¨ë¸ì— ëŒ€í•´ ì¶”ë¡  ìˆ˜í–‰
        for i, model_path in enumerate(MODEL_PATHS):
            # í˜„ì¬ epochì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
            model.load_state_dict(torch.load(model_path, map_location=DEVICE))
            
            # ì¶”ë¡ 
            logits = model(image_tensor)
            upsampled_logits = F.interpolate(logits, size=gt_mask_tensor.shape[-2:], mode='bilinear', align_corners=False)
            pred_mask = torch.argmax(upsampled_logits, dim=1).squeeze(0) # (1, H, W) -> (H, W)

            # NumPy ë°°ì—´ë¡œ ë³€í™˜ í›„ RGB ë§ˆìŠ¤í¬ ìƒì„± ë° ë¼ë²¨ ì¶”ê°€
            pred_mask_np = pred_mask.cpu().numpy()
            pred_mask_rgb = mask_to_rgb(pred_mask_np, palette)
            labeled_pred_img = add_label_to_image(Image.fromarray(pred_mask_rgb), EPOCH_LABELS[i])
            prediction_images.append(np.array(labeled_pred_img))
        
        # ì›ë³¸ ì´ë¯¸ì§€ì™€ ì •ë‹µ(GT) ë§ˆìŠ¤í¬ ì¤€ë¹„ ë° ë¼ë²¨ ì¶”ê°€
        img_np = (np.transpose(image_tensor.squeeze(0).cpu().numpy(), (1, 2, 0)) * 255).astype(np.uint8)
        gt_mask_np = gt_mask_tensor.numpy()
        gt_mask_rgb = mask_to_rgb(gt_mask_np, palette)

        labeled_original = add_label_to_image(Image.fromarray(img_np), "Original")
        labeled_gt = add_label_to_image(Image.fromarray(gt_mask_rgb), "Ground Truth")

        # [ì›ë³¸, ì •ë‹µ, ì˜ˆì¸¡1, ì˜ˆì¸¡2, ì˜ˆì¸¡3] ìˆœì„œë¡œ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        all_images = [np.array(labeled_original), np.array(labeled_gt)] + prediction_images
        
        # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ê°€ë¡œë¡œ ì—°ê²°
        comparison_img = np.concatenate(all_images, axis=1)

        # ìµœì¢… ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
        save_path = os.path.join(OUTPUT_DIR, f"progress_comparison_{saved_count + 1}.png")
        Image.fromarray(comparison_img).save(save_path)
        
        print(f"[{saved_count + 1}/{NUM_IMAGES_TO_SAVE}] ì§„í–‰ ê³¼ì • ë¹„êµ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {save_path}")
        saved_count += 1

print(f"\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ {saved_count}ê°œì˜ ì´ë¯¸ì§€ê°€ '{OUTPUT_DIR}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")