# GPU Device Setting

# pytorchë‚˜ transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ ë³´ë‹¤ ë¨¼ì € ì´ê²ƒì„ ê°€ìœ¼ì™€ì ¸ì˜¤ì•¼í•¨.
import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "1,2,3,4,5"

# SegFormer ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ì™„ì „í•œ ì½”ë“œ
import json
import torch
# torch.cuda.set_device(1)

from torch.utils.data import Dataset, DataLoader
from torch import nn
import evaluate
from datasets import load_dataset
from torchvision.transforms import ColorJitter
from transformers import (
    SegformerImageProcessor,
    SegformerForSemanticSegmentation,
    TrainingArguments,
    Trainer
)

from surface_dataset import SurfaceSegmentationDataset
# --- ì„¤ì • ---
# ì „ì²˜ë¦¬ëœ ë°ì´í„° ì •ë³´ê°€ ë‹´ê¸´ metadata.json íŒŒì¼ ê²½ë¡œ
metadata_path = "./processed_dataset/metadata.json"

# í•™ìŠµ íŒŒë¼ë¯¸í„°
batch_size = 64
target_size = (512, 512)
num_workers = 16  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì ˆ

"""
metadata.jsonì„ ì½ì–´ í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¡œë”ë¥¼ ìƒì„±
"""
with open(metadata_path, 'r', encoding='utf-8') as f:
    metadata = json.load(f)

# ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
train_dataset = SurfaceSegmentationDataset(metadata['train_data'], target_size, is_train=True)
valid_dataset = SurfaceSegmentationDataset(metadata['valid_data'], target_size, is_train=False)


filename = "id2label.json"

# í´ë˜ìŠ¤ ì´ë¦„ê³¼ ì¸ë±ìŠ¤ ë§¤í•‘ (ë°°ê²½ì€ 0)
class_to_idx = {
    'background': 0,
    'caution_zone': 1,
    'bike_lane': 2,
    'alley': 3,
    'roadway': 4,
    'braille_guide_blocks': 5,
    'sidewalk': 6
}

# id2label = ????
# id2label = {int(k): v for k, v in id2label.items()}
# label2id = {v: k for k, v in id2label.items()}
# num_labels = len(id2label)

# class_to_idxë¡œë¶€í„° id2labelê³¼ label2idë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
id2label = {int(idx): label for label, idx in class_to_idx.items()}
label2id = class_to_idx
num_labels = len(id2label)

print(f"í´ë˜ìŠ¤ ìˆ˜: {num_labels}")
print(f"í´ë˜ìŠ¤ ë ˆì´ë¸”: {id2label}")


# =============================================================================
# 2. ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ë° ë°ì´í„° ë³€í™˜ ì„¤ì •
# =============================================================================

processor = SegformerImageProcessor()

# data augmentation!!
jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)

print("ë°ì´í„° ë³€í™˜ ì„¤ì • ì™„ë£Œ")

# =============================================================================
# 3. ëª¨ë¸ ë¡œë“œ
# =============================================================================

pretrained_model_name = "nvidia/mit-b0"
model = SegformerForSemanticSegmentation.from_pretrained(
    pretrained_model_name,
    id2label=id2label,
    label2id=label2id
)

print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {pretrained_model_name}")
print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")

# =============================================================================
# 5. í›ˆë ¨ ì„¤ì •
# =============================================================================

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
epochs = 100
lr = 0.00006
batch_size = 64


# ë°ì´í„°ë¡œë” ìƒì„±
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

import torch.nn.functional as F

class DirectSegFormer(nn.Module):
    """
    ì§ì ‘ì ìœ¼ë¡œ í…ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” SegFormer
    """
    
    def __init__(self, pretrained_model_name="nvidia/mit-b0", num_classes=7):
        super().__init__()
        
        # ì›ë³¸ ëª¨ë¸ ë¡œë“œ
        self.original_model = SegformerForSemanticSegmentation.from_pretrained(
            pretrained_model_name,
            num_labels=num_classes,
            ignore_mismatched_sizes=True
        )
        
        # ì •ê·œí™” íŒŒë¼ë¯¸í„°
        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))
        
    def forward(self, x):
        """
        Args:
            x: (B, C, H, W) ì´ë¯¸ì§€ í…ì„œ (0-1 ë²”ìœ„)
        """
        # ì •ê·œí™”
        x = (x - self.mean) / self.std
        
        # ì›ë³¸ ëª¨ë¸ì˜ forward ë°©ì‹ ì‚¬ìš©
        # pixel_valuesë¥¼ ì§ì ‘ ì „ë‹¬
        outputs = self.original_model(pixel_values=x)
        
        return outputs.logits

    def predict(self, x):
        """ì˜ˆì¸¡ í•¨ìˆ˜"""
        self.eval()
        with torch.no_grad():
            logits = self.forward(x)
            predictions = F.softmax(logits, dim=1)
            pred_masks = torch.argmax(predictions, dim=1)
        return predictions, pred_masks


device= torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

model = DirectSegFormer(num_classes=7)
model.to(device)

import torch.optim as optim
# ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
optimizer = optim.AdamW(model.parameters(), lr=lr)
# ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„í• ì„ ìœ„í•œ í‘œì¤€ ì†ì‹¤ í•¨ìˆ˜
# ë ˆì´ë¸” ë§ˆìŠ¤í¬ëŠ” (B, H, W) í˜•íƒœì´ê³ , ê° í”½ì…€ ê°’ì€ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ì—¬ì•¼ í•¨
criterion = nn.CrossEntropyLoss()

import wandb
# wandb ì´ˆê¸°í™”
wandb.init(project="segmentation_project", name=f"run_0707")

model.train()
for epoch in range(epochs):
    running_loss = 0
    for i, data in enumerate(train_loader):
        images = data['pixel_values'].to(device)
        masks= data['labels'].to(device)

        # ëª¨ë¸ ìˆœì „íŒŒ -> ì‘ì€ í•´ìƒë„ì˜ logits ì¶œë ¥
        # logits.shape: [B, num_classes, H/4, W/4]
        logits = model(images)

        # ğŸš€ Logits ì—…ìƒ˜í”Œë§ (ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„) ğŸš€
        # upsampled_logits.shape: [B, num_classes, H, W]
        upsampled_logits = F.interpolate(
            logits,
            size=masks.shape[-2:],  # ì›ë³¸ ë§ˆìŠ¤í¬ì˜ (H, W) í¬ê¸°ë¡œ
            mode='bilinear',
            align_corners=False
        )

        optimizer.zero_grad()

        # CrossEntropyLossëŠ” (B, C, H, W) í˜•íƒœì˜ logitsì™€ (B, H, W) í˜•íƒœì˜ ë§ˆìŠ¤í¬ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
        loss = criterion(upsampled_logits, masks)
        loss.backward()
        optimizer.step()
        

        running_loss += loss.item()
        # if (i + 1) % 10 == 0:  # 10 ë°°ì¹˜ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
        print(f"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}")
        # break

    wandb.log({"loss": loss.item(), "epoch": epoch + 1, "step": i + 1})

    # ëª¨ë¸ ì €ì¥
    model_save_path = f"ckpts/seg_model_epoch_{epoch+1}.pth"
    torch.save(model.state_dict(), model_save_path)
    print(f"ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {model_save_path}")


    # ê°„ë‹¨í•œ ì¶”ë¡  (í•™ìŠµ í›„ í‰ê°€ ë˜ëŠ” ì‹œê°í™”ìš©)
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for val_data in valid_loader:  # val_loaderëŠ” ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨
            val_imgs = val_data['pixel_values'].to(device)
            val_masks = val_data['labels'].to(device)

            val_logits = model(val_imgs)

            val_outputs = F.interpolate(
                val_logits,
                size=masks.shape[-2:],  # ì›ë³¸ ë§ˆìŠ¤í¬ì˜ (H, W) í¬ê¸°ë¡œ
                mode='bilinear',
                align_corners=False
            )

            # loss ê³„ì‚°
            loss = criterion(val_outputs, val_masks)
            val_loss += loss.item()
            # wandbì— ì´ë¯¸ì§€ ë¡œê·¸ (ì²« ë°°ì¹˜ë§Œ ì˜ˆì‹œë¡œ ê¸°ë¡)
            # wandb.log({
            #     "val_input": [wandb.Image(val_imgs[0].cpu())],
            #     "val_pred": [wandb.Image(preds[0].cpu())],
            #     "val_mask": [wandb.Image(val_masks[0].cpu())]
            # })
            # break  # ì²« ë°°ì¹˜ë§Œ ì‹œê°í™”    

    # í‰ê·  ê²€ì¦ ì†ì‹¤ì„ wandbì— ê¸°ë¡
    avg_val_loss = val_loss / len(valid_loader)
    # wandb.log({"val_loss": avg_val_loss, "epoch": epoch + 1})
print("í•™ìŠµ ì™„ë£Œ!")

wandb.finish()

    