import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import SegformerForSemanticSegmentation
import inspect # forward ë©”ì†Œë“œ ì†ŒìŠ¤ ì½”ë“œë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ import

# ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— surface_dataset.pyê°€ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìžˆê±°ë‚˜
# íŒŒì´ì¬ ê²½ë¡œì— í¬í•¨ë˜ì–´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
from surface_dataset import SurfaceSegmentationDataset

# --- 0. í™˜ê²½ ì„¤ì • ---
# ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ðŸ”„ Using device: {device}")

# --- 1. ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì¤€ë¹„ ---
print("\nðŸ”„ 1. ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì¤€ë¹„")
metadata_path = "/home/work/data/indo_walking/surface_masking/processed_dataset/metadata.json"
data_base_path = os.path.dirname(os.path.dirname(metadata_path))

with open(metadata_path, 'r', encoding='utf-8') as f:
    metadata = json.load(f)

# ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
train_dataset = SurfaceSegmentationDataset(metadata['train_data'], target_size=(512, 512), is_train=True, data_base_path=data_base_path)

# ë°ì´í„°ë¡œë” ìƒì„±
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2) # ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” GPU ë©”ëª¨ë¦¬ì— ë§žê²Œ ì¡°ì ˆ

# --- 2. ëª¨ë¸ ì¤€ë¹„ ---
print("\nðŸ”„ 2. ëª¨ë¸ ì¤€ë¹„")
# ì‚¬ì „ í•™ìŠµëœ Segformer ëª¨ë¸ ë¡œë“œ
pretrained_model_name = "nvidia/segformer-b0-finetuned-ade-512-512"
pretrained_model_name = "nvidia/mit-b0"
model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model_name)

# --- í´ëž˜ìŠ¤ ê°œìˆ˜ ë³€ê²½ ë¡œì§ ---
# ë°ì´í„°ì…‹ì˜ í´ëž˜ìŠ¤ ê°œìˆ˜ì— ë§žê²Œ ëª¨ë¸ì˜ ìµœì¢… ë¶„ë¥˜ ë ˆì´ì–´ë¥¼ êµì²´
new_num_classes = 7 # ë°ì´í„°ì…‹ì—ì„œ í´ëž˜ìŠ¤ ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸°
decoder_hidden_size = model.decode_head.classifier.in_channels
model.decode_head.classifier = nn.Conv2d(decoder_hidden_size, new_num_classes, kernel_size=1)
model.config.num_labels = new_num_classes # ëª¨ë¸ ì„¤ì •ë„ ì—…ë°ì´íŠ¸

model.to(device)
model.eval()  # í‰ê°€ ëª¨ë“œ

print(f"ëª¨ë¸: {pretrained_model_name}")
print(f"ìˆ˜ì •ëœ í´ëž˜ìŠ¤ ê°œìˆ˜: {model.config.num_labels}")
print("-" * 50)


# --- 3. ì‹¤ì œ ë°ì´í„°ë¡œ Loss ë¹„êµ ---
print("ðŸš€ 3. ì‹¤ì œ ë°ì´í„°ë¡œ Loss ë¹„êµ ì‹œìž‘")

# ë°ì´í„°ë¡œë”ì—ì„œ ë°ì´í„° í•œ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
try:
    batch = next(iter(train_loader))
    pixel_values = batch['pixel_values'].to(device)
    labels = batch['labels'].to(device)
    print(f"ìž…ë ¥ ì´ë¯¸ì§€ shape: {pixel_values.shape}")
    print(f"ì •ë‹µ ë ˆì´ë¸” shape: {labels.shape}")
    print("-" * 50)

    # 3.1. ìžë™ ê³„ì‚° (Hugging Face ë‚´ìž¥ ë°©ì‹)
    print("ðŸš€ 3.1. Hugging Face ë‚´ìž¥ ë°©ì‹ìœ¼ë¡œ Loss ìžë™ ê³„ì‚°")
    with torch.no_grad():
        # labelsë¥¼ í•¨ê»˜ ì „ë‹¬í•˜ë©´ lossê°€ ìžë™ìœ¼ë¡œ ê³„ì‚°ë¨
        outputs_auto = model(pixel_values=pixel_values, labels=labels)

    auto_loss = outputs_auto.loss
    auto_logits = outputs_auto.logits

    print(f"ìžë™ ê³„ì‚°ëœ Loss: {auto_loss.item():.6f}")
    print(f"ìžë™ ê³„ì‚° ì‹œ ë°˜í™˜ëœ logits shape: {auto_logits.shape}")
    print("-" * 50)

    # 3.2. ìˆ˜ë™ ê³„ì‚° (ë‚´ë¶€ ë¡œì§ ìž¬í˜„)
    print("ðŸ› ï¸ 3.2. ë‚´ë¶€ ë¡œì§ì„ ìž¬í˜„í•˜ì—¬ Loss ìˆ˜ë™ ê³„ì‚°")

    # Logits ìƒì„± (labels ì—†ì´ ì „ë‹¬)
    with torch.no_grad():
        outputs_manual = model(pixel_values=pixel_values)
    manual_logits = outputs_manual.logits

    # Logits ì—…ìƒ˜í”Œë§
    upsampled_logits = F.interpolate(
        manual_logits,
        size=labels.shape[-2:], # ë ˆì´ë¸”ì˜ H, W í¬ê¸°ì— ë§žì¶¤
        mode='bilinear',
        align_corners=False
    )

    # CrossEntropyLoss ê³„ì‚°
    # surface_dataset.py êµ¬í˜„ì— ë”°ë¼ ignore_indexê°€ í•„ìš”í•  ìˆ˜ ìžˆìŒ
    # ë§Œì•½ íŠ¹ì • ê°’(ì˜ˆ: 255)ì„ ë¬´ì‹œí•´ì•¼ í•œë‹¤ë©´ ì•„ëž˜ì™€ ê°™ì´ ì„¤ì •
    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=255) 
    manual_loss = loss_fct(upsampled_logits, labels)

    print(f"ìˆ˜ë™ ê³„ì‚°ëœ Loss: {manual_loss.item():.6f}")
    print("-" * 50)

    # 3.3. ê²°ê³¼ ë¹„êµ
    print("âœ… 3.3. ìµœì¢… ê²°ê³¼ ë¹„êµ")
    are_losses_close = torch.allclose(auto_loss, manual_loss)
    print(f"ìžë™ ê³„ì‚° Lossì™€ ìˆ˜ë™ ê³„ì‚° Lossê°€ ì¼ì¹˜í•˜ëŠ”ê°€? -> {are_losses_close}")

    if are_losses_close:
        print("\nðŸŽ‰ ì„±ê³µ: ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œë„ ëª¨ë¸ì˜ ë‚´ë¶€ Loss ê³„ì‚° ê³¼ì •ì„ ì •í™•í•˜ê²Œ ìž¬í˜„í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ì‹¤íŒ¨: ìžë™ ê³„ì‚°ê³¼ ìˆ˜ë™ ê³„ì‚° Lossê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ignore_indexë‚˜ ë°ì´í„° íƒ€ìž…ì„ í™•ì¸í•´ë³´ì„¸ìš”.")

except StopIteration:
    print("ë°ì´í„°ë¡œë”ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì´ ë¹„ì–´ìžˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

breakpoint()

import inspect

# ëª¨ë¸ì˜ forward ë©”ì†Œë“œ ì†ŒìŠ¤ ì½”ë“œë¥¼ ì§ì ‘ ì¶œë ¥í•©ë‹ˆë‹¤.
print(inspect.getsource(model.forward))



# (Pdb) print(inspect.getsource(model.forward))
#     @auto_docstring
#     def forward(
#         self,
#         pixel_values: torch.FloatTensor,
#         labels: Optional[torch.LongTensor] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#         return_dict: Optional[bool] = None,
#     ) -> Union[tuple, SemanticSegmenterOutput]:
#         r"""
#         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):
#             Ground truth semantic segmentation maps for computing the loss. Indices should be in `[0, ...,
#             config.num_labels - 1]`. If `config.num_labels > 1`, a classification loss is computed (Cross-Entropy).

#         Examples:

#         ```python
#         >>> from transformers import AutoImageProcessor, SegformerForSemanticSegmentation
#         >>> from PIL import Image
#         >>> import requests

#         >>> image_processor = AutoImageProcessor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
#         >>> model = SegformerForSemanticSegmentation.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")

#         >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
#         >>> image = Image.open(requests.get(url, stream=True).raw)

#         >>> inputs = image_processor(images=image, return_tensors="pt")
#         >>> outputs = model(**inputs)
#         >>> logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)
#         >>> list(logits.shape)
#         [1, 150, 128, 128]
#         ```"""
        
"""
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )

        if labels is not None and self.config.num_labels < 1:
            raise ValueError(f"Number of labels should be >=0: {self.config.num_labels}")

        outputs = self.segformer(
            pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=True,  # we need the intermediate hidden states
            return_dict=return_dict,
        )

        encoder_hidden_states = outputs.hidden_states if return_dict else outputs[1]

        logits = self.decode_head(encoder_hidden_states)

        loss = None
        if labels is not None:
            # upsample logits to the images' original size
            upsampled_logits = nn.functional.interpolate(
                logits, size=labels.shape[-2:], mode="bilinear", align_corners=False
            )
            if self.config.num_labels > 1:
                loss_fct = CrossEntropyLoss(ignore_index=self.config.semantic_loss_ignore_index)
                loss = loss_fct(upsampled_logits, labels)
            elif self.config.num_labels == 1:
                valid_mask = ((labels >= 0) & (labels != self.config.semantic_loss_ignore_index)).float()
                loss_fct = BCEWithLogitsLoss(reduction="none")
                loss = loss_fct(upsampled_logits.squeeze(1), labels.float())
                loss = (loss * valid_mask).mean()

        if not return_dict:
            if output_hidden_states:
                output = (logits,) + outputs[1:]
            else:
                output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return SemanticSegmenterOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states if output_hidden_states else None,
            attentions=outputs.attentions,
        )
        
"""