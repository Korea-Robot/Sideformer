import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import cv2
import numpy as np
import json
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import os

# --- PyTorch ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ---
class SurfaceSegmentationDataset(Dataset):
    """ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€/ë§ˆìŠ¤í¬ ê²½ë¡œë¥¼ ë°›ì•„ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, data_list: List[Dict], target_size: Tuple[int, int], is_train: bool, data_base_path: str):
        self.data_list = data_list
        self.target_size = target_size
        self.is_train = is_train
        self.data_base_path = data_base_path # ë°ì´í„°ì…‹ì˜ ê¸°ë³¸ ê²½ë¡œ

        # ì´ë¯¸ì§€ ë³€í™˜ (í•™ìŠµ ì‹œì—ë§Œ Augmentation ì ìš©)
        self.image_transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize(self.target_size),
            transforms.RandomApply([
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1)
            ], p=0.5) if self.is_train else transforms.Lambda(lambda x: x),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def __len__(self) -> int:
        return len(self.data_list)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        # metadata.jsonì— ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ì½ê¸°
        item = self.data_list[idx]
        
        # ê¸°ë³¸ ê²½ë¡œì™€ ìƒëŒ€ ê²½ë¡œë¥¼ ì¡°í•©í•˜ì—¬ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        image_path = os.path.join(self.data_base_path, item['image_path'])
        mask_path = os.path.join(self.data_base_path, item['mask_path'])

        # ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)
        if mask is None:
            raise FileNotFoundError(f"ë§ˆìŠ¤í¬ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {mask_path}")

        # ì´ë¯¸ì§€ ë³€í™˜ ì ìš©
        image = self.image_transform(image)

        # ë§ˆìŠ¤í¬ ë¦¬ì‚¬ì´ì¦ˆ ë° í…ì„œ ë³€í™˜
        mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)
        mask = torch.from_numpy(mask).long()  # (H, W), Long íƒ€ì…ìœ¼ë¡œ ë³€í™˜

        return {'pixel_values': image, 'labels': mask}


# --- ë°ì´í„°ë¡œë” ìƒì„± í•¨ìˆ˜ ---
def create_dataloaders(metadata_path: str, batch_size: int, target_size: Tuple[int, int], num_workers: int) -> Tuple[DataLoader, DataLoader, Dict]:
    """
    metadata.jsonì„ ì½ì–´ í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¡œë”ë¥¼ ìƒì„±
    """
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    # metadata.json íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ êµ¬í•¨
    data_base_path = os.path.dirname(os.path.dirname(metadata_path)) # processed_datasetì˜ ìƒìœ„ í´ë”

    # ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    train_dataset = SurfaceSegmentationDataset(metadata['train_data'], target_size, is_train=True, data_base_path=data_base_path)
    valid_dataset = SurfaceSegmentationDataset(metadata['valid_data'], target_size, is_train=False, data_base_path=data_base_path)

    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    return train_loader, valid_loader, metadata


# --- ì‹œê°í™” í•¨ìˆ˜ (ê²°ê³¼ í™•ì¸ìš©) ---
def visualize_sample(dataset: Dataset, idx: int, metadata: Dict):
    """ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œì„ ì‹œê°í™”í•˜ì—¬ ë°ì´í„° ë¡œë”©ì´ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    class_colors = {int(k): v for k, v in metadata['class_colors'].items()}
    sample = dataset[idx]
    image_tensor = sample['pixel_values']
    mask_tensor = sample['labels']

    # ì´ë¯¸ì§€ í…ì„œ Denormalize
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
    image = image_tensor * std + mean
    image = image.permute(1, 2, 0).numpy().clip(0, 1)

    # ë§ˆìŠ¤í¬ í…ì„œë¥¼ ì»¬ëŸ¬ ì´ë¯¸ì§€ë¡œ ë³€í™˜
    mask_colored = np.zeros((*mask_tensor.shape, 3), dtype=np.uint8)
    for class_idx, color in class_colors.items():
        mask_colored[mask_tensor == class_idx] = color

    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    axes[0].imshow(image)
    axes[0].set_title('Image')
    axes[0].axis('off')
    axes[1].imshow(mask_colored)
    axes[1].set_title('Mask')
    axes[1].axis('off')
    # plt.show()
    savefig('sampleimagemaskdata.png')


# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    # --- ì„¤ì • ---
    # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì •ë³´ê°€ ë‹´ê¸´ metadata.json íŒŒì¼ ê²½ë¡œ
    METADATA_PATH = "/home/work/data/indo_walking/surface_masking/processed_dataset/metadata.json"
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    BATCH_SIZE = 8
    TARGET_SIZE = (512, 512)
    NUM_WORKERS = 4  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì ˆ

    # --- ì‹¤í–‰ ---
    try:
        print(f"'{METADATA_PATH}' íŒŒì¼ì„ ì´ìš©í•´ ë°ì´í„°ë¡œë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        train_loader, valid_loader, metadata = create_dataloaders(
            metadata_path=METADATA_PATH,
            batch_size=BATCH_SIZE,
            target_size=TARGET_SIZE,
            num_workers=NUM_WORKERS
        )

        print("\nğŸ‰ ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ!")
        print(f"  - í•™ìŠµìš© ë°ì´í„° ìˆ˜: {len(train_loader.dataset)}")
        print(f"  - ê²€ì¦ìš© ë°ì´í„° ìˆ˜: {len(valid_loader.dataset)}")
        print(f"  - í•™ìŠµìš© ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
        print(f"  - ê²€ì¦ìš© ë°°ì¹˜ ìˆ˜: {len(valid_loader)}")

        # ì²« ë²ˆì§¸ í•™ìŠµ ë°°ì¹˜ ì •ë³´ í™•ì¸
        print("\n-- ì²« í•™ìŠµ ë°°ì¹˜ ìƒ˜í”Œ ì •ë³´ --")
        sample_batch = next(iter(train_loader))
        print(f"  - ì´ë¯¸ì§€ ë°°ì¹˜ í˜•íƒœ: {sample_batch['pixel_values'].shape}")
        print(f"  - ë§ˆìŠ¤í¬ ë°°ì¹˜ í˜•íƒœ: {sample_batch['labels'].shape}")

        mask_sample = sample_batch['labels']

        # breakpoint()

        # ê²€ì¦ ë°ì´í„°ì…‹ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œ ì‹œê°í™”ë¡œ í™•ì¸
        print("\n[ê²°ê³¼ í™•ì¸] ê²€ì¦ ë°ì´í„°ì…‹ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œì„ ì‹œê°í™”í•©ë‹ˆë‹¤...")
        visualize_sample(dataset=valid_loader.dataset, idx=0, metadata=metadata)

    except FileNotFoundError as e:
        print(f"ğŸš¨ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(e)
        print("ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"ğŸš¨ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")